{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import codecs\n",
    "import folium\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (20,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data_clean'\n",
    "IS_DATE = re.compile(\"^[1-2]{1}[0-9]{3}$\") #will be useful to detect values that correspond to dates\n",
    "\n",
    "PATH = 'docs/_stories/'\n",
    "PLOT_FOLDER = 'plots/'\n",
    "DF_FOLDER = 'dataframes/'\n",
    "MAP_FOLDER = 'maps/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to quickly describe a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describeDf(df,name=\"DESCRIPTION\",level=2):\n",
    "    \"\"\"\n",
    "    Describe the data. Different levels are possible.\n",
    "    Level 0: Print the title of the dataset\n",
    "    Level 1: Name the different columns\n",
    "    Level 2: Explore the values of each column\n",
    "    \"\"\"\n",
    "    date=[]\n",
    "    \n",
    "    # Explore each column of the dataset\n",
    "    if(level==2):\n",
    "        # Print a header\n",
    "        print(\"______________________________ \" + name+\" ______________________________\\n\")\n",
    "        print(\"\\n\")\n",
    "        for col in df.columns:\n",
    "            # Print the attributes of each column\n",
    "            if((not IS_DATE.match(col)) & (not re.compile(\"[0-9]+ an\").match(col))):\n",
    "                print(\"         ATTRIBUTE: \"+col)\n",
    "                print(\"   \"+str(df[col].unique()))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                date.append(col)\n",
    "        # Print the years we have information on\n",
    "        print(\"         VALUES: \"+str(date))\n",
    "        print(\"\\n\\n\\n\")\n",
    "        return\n",
    "    \n",
    "    # Recover and print the columns of the datasets\n",
    "    elif(level==1):\n",
    "        # Print a header\n",
    "        print(\"______________________________ \" + name+\" ______________________________\\n\")\n",
    "        val = []\n",
    "        # Print the different columns\n",
    "        for col in df.columns:\n",
    "            if((not IS_DATE.match(col)) & (not re.compile(\"[0-9]+ an\").match(col))):\n",
    "                val.append(col)\n",
    "        print(val)\n",
    "        print(\"\\n\\n\\n\")\n",
    "        return\n",
    "    \n",
    "    # Simply print the name of the dataset\n",
    "    elif(level==0):\n",
    "        print(\"   >  \" + name+\"\\n\")\n",
    "    \n",
    "    # Sanity check: the given level does exist\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to restrict the dataframe to a fix period in term of years\n",
    "\n",
    "We need this function as our different datasets are often based on different yearly periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period(df, start=None,end=None):\n",
    "    \"\"\"\n",
    "    Function to keep the columns of interest of a dataset, between a starting and an ending date.\n",
    "    \"\"\"\n",
    "    columns_to_keep = []\n",
    "    dates = []\n",
    "    \n",
    "    # Go through the columns\n",
    "    for elem in df.columns:\n",
    "        # If it is not a date we keep it\n",
    "        if not IS_DATE.match(elem):\n",
    "            columns_to_keep.append(elem)\n",
    "        # Else we store it to see later if we keep it\n",
    "        else:\n",
    "            dates.append(int(elem))\n",
    "    \n",
    "    # Define the starting and ending dates in the case they are not\n",
    "    if start == None:\n",
    "        start = np.min(dates)\n",
    "    if end == None:\n",
    "        end = np.max(dates)\n",
    "    \n",
    "    # Only keep the columns of interest\n",
    "    for date in dates:\n",
    "        if (date<=end) & (date>=start):\n",
    "            columns_to_keep.append(str(date))\n",
    "    \n",
    "    return df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the age corresponding to the intervals as input\n",
    "\n",
    "Using this functions will allow us to treat people between 40 and 50 years old as one group for example. Again, this is needed because of the disparity among the datasets, with some of them considering each age category differently, while others  groupped the people in different intervals. This functions allow us to compare and work with those different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_int_to_age(k,age_max):\n",
    "    \"\"\"\n",
    "    Transform an integer to what is define as an age, i.e. 20 to '20 ans' ('20 years old')\n",
    "    \"\"\"\n",
    "    if(k != 1 and k<age_max):\n",
    "        return '{} ans'.format(k)\n",
    "    if(k == 1):\n",
    "        return '{} an'.format(k)\n",
    "    if(k==age_max):\n",
    "        if (age_max == 100):\n",
    "            return '{} ans ou plus'.format(age_max)\n",
    "        else:\n",
    "            return '{} ans et plus'.format(age_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_age(intervals,age_max):\n",
    "    \"\"\"\n",
    "    INPUT:  intervals: Array corresponding to the delimitation of the wanted intervals.\n",
    "            age_max: The maximal age to consider.\n",
    "    \"\"\"\n",
    "    ages_total = []\n",
    "    \n",
    "    # Build the intervals\n",
    "    for i in range(len(intervals)-1):\n",
    "        ages = []\n",
    "        \n",
    "        # We keep the first limit of the interval as its first element: this will\n",
    "        # allow us to identify the different intervals later\n",
    "        ages.append(intervals[i])\n",
    "        \n",
    "        # Put the corresponding ages in the interval\n",
    "        for k in range(int(intervals[i]),int(intervals[i+1])):\n",
    "            # Use our function\n",
    "            age = map_int_to_age(k,age_max)\n",
    "            ages.append(age)\n",
    "            if(k==100):\n",
    "                break\n",
    "                \n",
    "        # Store the interval we just created\n",
    "        ages_total.append(ages)\n",
    "    \n",
    "    return ages_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping for the cantons\n",
    "\n",
    "One recurrent problem we encountered, as presented in introduction, is the fact that different datasets have information in different languages. In general, we can treat them case by case, but it becomes a major issue when it comes to the Swiss cantons. As we wanted to do an in depth analysis and comparison of them, we needed to be able to refer them in different datasets.\n",
    "\n",
    "We thus decided to build a dictionnary of every different mentioning of the cantons we found, with a little function to get the corresponding key, allowing us to compare cantons in different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cantons = dict()\n",
    "dict_cantons['AA'] = ['Appenzell Rh.-Ext.','Appenzell Ausserrhoden']\n",
    "dict_cantons['AG'] = ['Argovie','Aargau']\n",
    "dict_cantons['AI'] = ['Appenzell Rh.-Int.','Appenzell Innerrhoden']\n",
    "dict_cantons['BE'] = ['Berne', 'Bern', 'Bern / Berne']\n",
    "dict_cantons['BL'] = ['Bâle-Campagne','Basel-Landschaft']\n",
    "dict_cantons['BS'] = ['Bâle-Ville','Basel-Stadt']\n",
    "dict_cantons['FR'] = ['Fribourg', 'Freiburg', 'Fribourg / Freiburg']\n",
    "dict_cantons['GE'] = ['Genève', 'Genf']\n",
    "dict_cantons['GL'] = ['Glaris', 'Glarus']\n",
    "dict_cantons['GR'] = ['Grisons', 'Graubünden', 'Grigioni', 'Grischun', 'Graubünden / Grigioni / Grischun']\n",
    "dict_cantons['JU'] = ['Jura']\n",
    "dict_cantons['LU'] = ['Lucerne', 'Luzern']\n",
    "dict_cantons['NE'] = ['Neuchâtel']\n",
    "dict_cantons['NW'] = ['Nidwald', 'Nidwalden']\n",
    "dict_cantons['OW'] = ['Obwald', 'Obwalden']\n",
    "dict_cantons['SG'] = ['St. Gall', 'St. Gallen', 'Saint-Gall']\n",
    "dict_cantons['SH'] = ['Schaffhouse', 'Schaffhausen']\n",
    "dict_cantons['SO'] = ['Soleure', 'Solothurn']\n",
    "dict_cantons['SZ'] = ['Schwytz', 'Schwyz']\n",
    "dict_cantons['TE'] = ['Tessin', 'Ticino']\n",
    "dict_cantons['TG'] = ['Thurgovie', 'Thurgau']\n",
    "dict_cantons['UR'] = ['Uri']\n",
    "dict_cantons['VS'] = ['Valais', 'Wallis', 'Valais / Wallis']\n",
    "dict_cantons['VD'] = ['Vaud']\n",
    "dict_cantons['ZG'] = ['Zoug', 'Zug']\n",
    "dict_cantons['ZH'] = ['Zurich', 'Zürich']\n",
    "\n",
    "\n",
    "def getKeysByValue(dictOfElements, valueToFind):\n",
    "    '''\n",
    "    Find the key of a canton.\n",
    "    '''\n",
    "    for item  in dictOfElements.items():\n",
    "        if valueToFind==item[0]:\n",
    "            return valueToFind\n",
    "        if valueToFind in item[1]:\n",
    "            return item[0]\n",
    "            break\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implemented the following function to change all the cantons names to their key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cantons_names(x):\n",
    "    clean = x.copy()\n",
    "    for cant in clean['canton'].unique():\n",
    "        # Check if what is recorded as a 'canton' really is a canton, i.e. is in our\n",
    "        # dictionary (this doesn't keep the Swiss totals for example)\n",
    "        try:\n",
    "            getKeysByValue(dict_cantons, cant)\n",
    "        # If not, we don't keep the rows with those 'cantons'\n",
    "        except:\n",
    "            clean = clean[clean.canton!=cant]\n",
    "\n",
    "    # Replace the cantons name by their keys to compare them below\n",
    "    clean['canton'] = [getKeysByValue(dict_cantons, cant) for cant in clean['canton']]\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the list of the cantons keys to be able to put every dataframe in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cantons_keys = []\n",
    "for key in dict_cantons.keys():\n",
    "    list_cantons_keys.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot a set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe(x, y=None, title=None, xlabel=None, ylabel=None, labels=None,\\\n",
    "                   rotation=0, number_to_plot=None, bars=None,name_save=None):\n",
    "    if number_to_plot == None:\n",
    "        try:\n",
    "            number_to_plot = y.shape[0]\n",
    "        except:\n",
    "            pass\n",
    "    if bars == None:\n",
    "        plt.figure(figsize=(12,7))\n",
    "        for j in range(number_to_plot):\n",
    "            plt.plot(x, y.iloc[j], label = labels[j])\n",
    "    else:\n",
    "        x.plot(kind=bars, figsize=(12,7))\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.legend(fontsize=10, loc=0, bbox_to_anchor=(1,1))\n",
    "    plt.xticks(rotation=rotation, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    if (name_save!=None):\n",
    "        plt.savefig(PATH+PLOT_FOLDER+name_save+'.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to scatter plot correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(x, y, title=None, xlabel=None, ylabel=None, labels=None,\\\n",
    "                   rotation=0, number_to_plot=None,name_save=None):\n",
    "    if number_to_plot == None:\n",
    "        try:\n",
    "            number_to_plot = y.shape[0]\n",
    "        except:\n",
    "            pass\n",
    "    plt.figure(figsize=(12,7))\n",
    "    for j in range(number_to_plot):\n",
    "        plt.scatter(x.iloc[j], y.iloc[j], label = labels[j])\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.legend(fontsize=10, loc=\"best\", bbox_to_anchor=(1,1))\n",
    "    plt.xticks(rotation=rotation, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    if (name_save!=None):\n",
    "        plt.savefig(PATH+PLOT_FOLDER+name_save+'.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to save results in order to put in the datastory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_it(data, name):\n",
    "    if (type(data) == type(pd.DataFrame())):\n",
    "        with codecs.open(PATH+DF_FOLDER+name+'.html','w',\"utf-8\") as f: \n",
    "            f.write(data.to_html())\n",
    "    if (type(data) == type(folium.Map())):\n",
    "        data.save(PATH+MAP_FOLDER+name+\".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .Datasets Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, to perform our analysis, we took many datasets from the Swiss OpenData website. All these datasets are excels spreedsheet in differents forms, we hence needed to do a pre-treatment to transform them into clear `csv`\n",
    " files. They are stored in the folder `data_clean`.\n",
    "\n",
    "The pre-treatment was huge since the datasets were sometimes split into multiple spreedsheets (on for each year) for example. Moreover, the excel files were organized to be visually good looking, with many merged cells and various hierachical levels in the data, thus requiring more work to clean and ready it for the future processings with pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot of our different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DATAFRAMES AVAILABLES:\n",
      "\n",
      "   >  df_Accident_cantons\n",
      "\n",
      "   >  df_Accident_circonstances\n",
      "\n",
      "   >  df_Accident_objets\n",
      "\n",
      "   >  df_Accident_type_route\n",
      "\n",
      "   >  df_Accident_victimes\n",
      "\n",
      "   >  df_Besoin_sante\n",
      "\n",
      "   >  df_Depense_menages_canton\n",
      "\n",
      "   >  df_Frais_routes_cantonales\n",
      "\n",
      "   >  df_Frais_routes_communales\n",
      "\n",
      "   >  df_Frais_routes_nationales\n",
      "\n",
      "   >  df_hacked_accident\n",
      "\n",
      "   >  df_Longueur_routes\n",
      "\n",
      "   >  df_Population_2010\n",
      "\n",
      "   >  df_Population_age_1992\n",
      "\n",
      "   >  df_Population_canton_1992\n",
      "\n",
      "   >  df_Proportion_permis\n",
      "\n",
      "   >  df_Qualite_vie_agglo\n",
      "\n",
      "   >  df_Recettes_routes\n",
      "\n",
      "   >  df_Voitures\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "print(\"  DATAFRAMES AVAILABLES:\\n\")\n",
    "\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    # Load the filename\n",
    "    filename = os.fsdecode(file)\n",
    "    # Name the corresponding DataFrame \"df_.....\"\n",
    "    tablename = \"df_\"+os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    # Load the values in the DataFrame\n",
    "    globals()[tablename] = pd.read_csv(DATA_FOLDER + '/' + filename)\n",
    "    try:\n",
    "        globals()[tablename] = globals()[tablename].drop(columns=['Unnamed: 0'])\n",
    "    except:\n",
    "        pass\n",
    "    # Print the title of the datasets\n",
    "    describeDf(globals()[tablename], name=tablename,level=0)\n",
    "    dataframes[tablename] = globals()[tablename] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
